{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step01 -- Initializing Spark\n",
    "\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.0 pyspark-shell'\n",
    "from pyspark import SparkContext, SparkConf \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# We add this line to avoid an error : \"Cannot run multiple SparkContexts at once\". \n",
    "# If there is an existing spark context, we will reuse it instead of creating a new context.\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# local[*]: run Spark locally with as many working processors as logical cores on your machine.\n",
    "# In the field of `master`, we use a local server with as many working processors (or threads) as possible (i.e. `local[*]`). \n",
    "# If there is no existing spark context, we now create a new context\n",
    "if (sc is None):\n",
    "    sc = SparkContext(master=\"local[*]\")\n",
    "spark = SparkSession(sparkContext=sc).builder.appName(\"FIT5202 Assignment 1 Part-B\") \\\n",
    "        .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/FIT5202.crime\") \\\n",
    "        .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/FIT5202.crime\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 02 -- Create a Dataframe\n",
    "#Renaming Crime_Statistics_SA_2010_present as \"Crime\"\n",
    "\n",
    "#In this question I'm using spark session variable to create a dataframe\n",
    "\n",
    "crimeDataFrame = spark.read.csv('Crime.csv',inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 03 -- Writing into MongoDB with overwrite mode\n",
    "\n",
    "crimeDataFrame.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 04 -- Reading it from MongoDB\n",
    "\n",
    "newDataFrame = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n",
    "newDataFrame.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 05 -- Statistics of numeric and string column\n",
    "\n",
    "#Describe function to provide the summary of the dataframe\n",
    "\n",
    "dfDescribe = newDataFrame.describe('Reported Date','Offence Count')\n",
    "dfDescribe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing null value from Reported Date in order to use it for further questions\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "newDataFrame=newDataFrame.where(col('Reported Date').isNotNull())\n",
    "newDataFrame.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 06 - Changing String type to Date using UDF\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "convertToDate = udf (lambda x: datetime.strptime(x, '%d/%m/%Y'), DateType())\n",
    "newdf = newDataFrame.withColumn('Reported Date',convertToDate(col('Reported Date')))\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 07 - Preliminary Data Analysis\n",
    "#1 Number of level 2 offence and list of level 2 offence\n",
    "\n",
    "lvl2Offence=newdf.select('Offence Level 2 Description').where(col('Offence Level 2 Description').isNotNull()).distinct()\n",
    "print(\"Number of Level 2 offences are:\",lvl2Offence.count())\n",
    "\n",
    "lvl2Offence.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 07 - Preliminary Data Analysis\n",
    "#2 Number of offences against person\n",
    "\n",
    "personOffence=newdf.select('Offence Count')\\\n",
    ".where(col('Offence Level 1 Description') == 'OFFENCES AGAINST THE PERSON')\\\n",
    ".groupBy().sum()\\\n",
    ".withColumnRenamed('sum(Offence Count)','Sum')\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 07 - Preliminary Data Analysis\n",
    "#3 Number of serial criminal tresspasses with more than 1 offence count\n",
    "\n",
    "trespassCount=newdf.filter((col('Offence Level 2 Description') == 'SERIOUS CRIMINAL TRESPASS') & (col('Offence Count') > 1))\n",
    "print(\"Number of serial criminal tresspasses with more than 1 offence count is:\",trespassCount.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 07 - Preliminary Data Analysis\n",
    "#4 percentage of crimes against property\n",
    "\n",
    "totalCount=newDataFrame.count()\n",
    "propertyOffence=newdf.filter(col('Offence Level 1 Description') == 'OFFENCES AGAINST PROPERTY').count()\n",
    "print(\"Percentage of crimes that are against property is: {}%\".format(round(((propertyOffence/totalCount)*100),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 8.1 - Exploratory data analysis\n",
    "#1 Crimes per year\n",
    "# using year function to extract the year from date\n",
    "\n",
    "from pyspark.sql.functions import year\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "bar_width = 0.5\n",
    "\n",
    "yearList=('2010','2011','2012','2013','2014','2015','2016','2017','2018','2019')\n",
    "crimesPerYear=newdf.select(year('Reported Date').alias('Year'),'Offence Count')\\\n",
    ".groupBy('Year').sum()\\\n",
    ".orderBy('Year').collect()\n",
    "\n",
    "crimeYear= [row['Year'] for row in crimesPerYear]\n",
    "crimeCount= [row['sum(Offence Count)'] for row in crimesPerYear]\n",
    "\n",
    "plt.subplots(figsize=(10,5))\n",
    "plt.style.use('ggplot')\n",
    "plt.bar(crimeYear,crimeCount,bar_width, align='center', color='C0')\n",
    "plt.xticks(crimeYear,yearList)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Count')\n",
    "plt.title('CRIMES PER YEAR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 1 ANALYSIS\n",
    "\n",
    "The bar chart illustrates number of crimes happened between 2010-2019. It can be seen that number of crimes spiked\n",
    "between 2010 and 2011,however crimes started to dwindle from 2011 to 2014. In the year of 2015 and 2016, number of crimes recorded are almost equal and then the numbers started to decline in heavy margin after 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 8.2 - Exploratory data analysis\n",
    "#2  Crimes per month\n",
    "# using month function to extract the month from date\n",
    "\n",
    "from pyspark.sql.functions import month\n",
    "%matplotlib inline\n",
    "bar_width = 0.5\n",
    "\n",
    "crimesPerMonth=newdf.select(month('Reported Date').alias('Month'),'Offence Count')\\\n",
    ".groupBy('Month').sum('Offence Count').orderBy('Month')\\\n",
    ".collect()\n",
    "\n",
    "monthsList=('Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec')\n",
    "#crimeMonth=np.arange(len(monthsList))\n",
    "crimeMonth= [row['Month'] for row in crimesPerMonth]\n",
    "crimeTotalCount= [row['sum(Offence Count)'] for row in crimesPerMonth]\n",
    "\n",
    "plt.subplots(figsize=(10,5))\n",
    "plt.bar(crimeMonth,crimeTotalCount,bar_width, align='center', color='C0')\n",
    "plt.xticks(crimeMonth,monthsList)\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Count')\n",
    "plt.title('CRIMES PER MONTH')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 ANALYSIS\n",
    "\n",
    "The bar chart illustrates number of crimes happened over a period of 12 months.We can see in the month of January and \n",
    "October crimes are crossing 70,000 mark, on the other hand April and June recorded lowest crime rates ever. The pattern\n",
    "what we observe from this bar chart is that crimes are distributed in all 12 months. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 8.3 - Exploratory data analysis\n",
    "#3 Top 20 suburbs with high Crime rates\n",
    "# using concat function to merge two columns(Suburb and Postcode)\n",
    "\n",
    "from pyspark.sql.functions import concat,lit\n",
    "%matplotlib inline\n",
    "\n",
    "crimesPerSuburb=newdf.select(concat(col('Suburb - Incident'), lit('-'), col('Postcode - Incident')).alias('Suburb'),'Offence Count')\\\n",
    ".where(col('Suburb - Incident').isNotNull())\\\n",
    ".groupBy('Suburb').sum()\\\n",
    ".sort('sum(Offence Count)', ascending=False)\\\n",
    ".take(20)\n",
    "\n",
    "crimeSuburb= [row['Suburb'] for row in crimesPerSuburb]\n",
    "crimeSuburbCount= [row['sum(Offence Count)'] for row in crimesPerSuburb]\n",
    "\n",
    "plt.subplots(figsize=(10,5))\n",
    "plt.bar(crimeSuburb,crimeSuburbCount,bar_width, align='center', color='C0')\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.xlabel('Suburb')\n",
    "plt.ylabel('Count')\n",
    "plt.title('CRIMES PER SUBURB')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 ANALYSIS\n",
    "\n",
    "The bar chart illustrates top 20 suburbs with high crime rates. Adelaide suburb with postcode 5000 has the highest\n",
    "number of crimes reported with 50k approximately while other suburbs has maintained consistency of not crossing 15k mark. Data skewness can be observed, but it's negligible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 8.4 - Exploratory data analysis\n",
    "#4 SERIOUS CRIMINAL TRESPASS per day and month\n",
    "# using dayofweek function to find the day crime was recorded\n",
    "\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import dayofweek\n",
    "%matplotlib inline\n",
    "\n",
    "width=0.5\n",
    "\n",
    "#Criminal count for SERIOUS CRIMINAL TRESPASS per day\n",
    "\n",
    "crimesPerDay = newdf.select(dayofweek('Reported Date').alias('Day'),'Offence Level 2 Description','Offence Count')\\\n",
    ".where(col('Offence Level 2 Description') == 'SERIOUS CRIMINAL TRESPASS')\\\n",
    ".groupBy('Day').sum()\\\n",
    ".orderBy('Day')\\\n",
    ".collect()\n",
    "\n",
    "\n",
    "crimeDay = [row['Day'] for row in crimesPerDay]\n",
    "crimeDayCount = [row['sum(Offence Count)'] for row in crimesPerDay]\n",
    "y_crimepos = np.arange(len(crimeDay))\n",
    "\n",
    "#Criminal count for SERIOUS CRIMINAL TRESPASS per month\n",
    "\n",
    "tresspassPerMonth = newdf.select(month('Reported Date').alias('Month'),'Offence Level 2 Description','Offence Count')\\\n",
    ".where(col('Offence Level 2 Description') == 'SERIOUS CRIMINAL TRESPASS')\\\n",
    ".groupBy('Month').sum()\\\n",
    ".orderBy('Month')\\\n",
    ".collect()\n",
    "\n",
    "tresspassMonth = [row['Month'] for row in tresspassPerMonth]\n",
    "tresspassMonthCount = [row['sum(Offence Count)'] for row in tresspassPerMonth]\n",
    "y_pos = np.arange(len(tresspassMonth))\n",
    "\n",
    "#Plotting graphs\n",
    "\n",
    "plt.subplots(figsize=(10,5))\n",
    "plt.bar(y_crimepos + 1, crimeDayCount, bar_width, align='center', color='blue',label='DAY')\n",
    "plt.bar(y_pos + 1 + width, tresspassMonthCount, bar_width, align='center', color='red',label='MONTH')\n",
    "plt.xlabel('Day & Month')\n",
    "plt.ylabel('Count')\n",
    "plt.title('SERIOUS CRIMINAL TRESPASSES PER DAY AND MONTH')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 ANALYSIS\n",
    "\n",
    "The bar chart indicates number of serious criminal trespasses per day and month from 2010 to 2019.Blue bars indicates days of the week starting from sunday to saturday and red bars indicating months from january to december. \n",
    "It can be noticed that tuesdays had several crimes compared to other days. Similary the number of crimes recorded in\n",
    "January and October are huge. From my observation, high crimes are committed in the months of January,March, October \n",
    "especially on tuesdays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
